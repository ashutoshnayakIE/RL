{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Dynamic Programming</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming has a long history before the popularity of Reinforcement Learning. Dynamic programming is used to take sequentially actions based on past decisions, current conditions, expected future rewards. We will consider Markov Decision Process (MDP) where the current conditions is brought upon by the decisions made in the past and change how the environment reacted to those decisions (we cannot go back in time and change our decisions). In MDP, we take decision based on current conditions and what can we do get maximum expected reward (not just immediate but long term reward). To simplify, we will consider discrete time environment where actions are taken in discrete time slots rather than continuous time. If the system is continuous, we can always time discrete (for example, take decisions every 5 minutes). In driving, for example, we take decisions when we reach a city (node in our model).\n",
    "\n",
    "THE books on Dynamic Programming (DP) and Reinforcement Learning (RL) are:\n",
    "1. http://athenasc.com/dpbook.html  (DP)\n",
    "2. http://www.incompleteideas.net/book/the-book-2nd.html (RL)\n",
    "\n",
    "These books are very well written, very easy to follow and understand the concepts. This tutorial just brings some examples to explain the concepts in DP textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this brief introduction, we will go through some examples of simple dynamic programming problems and lay a background for model day Reinforcement Learning. First, the most important figure for all of RL. In DP, the agent is the user (manual or automated) who takes decisions for the next step.\n",
    "\n",
    "<img src='figures/RL.png'></img>\n",
    "<center>Figure 1. Reinforcement Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure explains DP mathematically as what we a re trying to achieve when taking sequential decisions. We aim for long term rewards, accumulated over time (for example, winning at the end of the game and not caring about immediate reward like taking out a pawn of the opponent but actually exposing to opponent). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
